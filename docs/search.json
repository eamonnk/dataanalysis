[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to some data analysis projects",
    "section": "",
    "text": "This site shows some Jupyter notebooks covering various data analysis topics\nit is hosted using Quarto and GitHub Pages."
  },
  {
    "objectID": "notebooks/esg_text_mining_poc_final.html",
    "href": "notebooks/esg_text_mining_poc_final.html",
    "title": "Company Ethical Check",
    "section": "",
    "text": "Using Data Analysis and Data Classification to determine if a company’s values match your own."
  },
  {
    "objectID": "notebooks/esg_text_mining_poc_final.html#step-5---classification-models",
    "href": "notebooks/esg_text_mining_poc_final.html#step-5---classification-models",
    "title": "Company Ethical Check",
    "section": "Step 5 - Classification Models",
    "text": "Step 5 - Classification Models\n5.1 - Split dataset into features and target labels\n\n# define good companies and bad companies#\n# sorting into positive and negative companies based off existing industry esg ratings\n# as have fata already extracted froim files will just assign exisint gdat based ont hen company key in the data\n# won't use load_files method as haver dat already extracted\n\ngood_companies = {\"kerrygroup\", \"crh\", \"smurfitkappa\", \"kingspan\"}\nbad_companies = {\"exxonmobil\", \"jbs\", 'aramco'}\n\n#contains processed cleaned report text from \nX_texts = []\ny_labels = []\n\n\n# assign positive y label to good companies\n# 1 is positive &gt; 0 is negative\n# we have two data set_matplotlib_closeprocessed_per_file_data = contains &gt; \"company\": company,  \"filename\": pdf_file.name,  \"text\": cleaned &gt;&gt;&gt; all data is separate dout per file and company key-value pairs\n# combined_per_comp_text = contains &gt; [record[\"company\"]] += \" \" + record[\"text\"] &gt;&gt;&gt; all data is combined across companies\n# we'll use the first as we have it separated out at file level giving us more elements to train with\nfor record in processed_per_file_data:\n    company = record['company'].lower()\n    text = record['text']\n    \n    \n    if company.lower() in good_companies:\n        X_texts.append(text)\n        y_labels.append(1)\n\n    elif company.lower() in bad_companies:\n        X_texts.append(text)\n        y_labels.append(0)\n    \n\n5.2 - Verify variables are as expoected\n\n# quick check tomake sure we're using the right variables and they are what we expect going into the models\nprint(\"Total documents:\", len(X_texts))  # Should be 43\nprint(\"Good ESG count:\", y_labels.count(1))  # Should be 33\nprint(\"Bad ESG count:\", y_labels.count(0))  # Should be 10\n\nTotal documents: 43\nGood ESG count: 33\nBad ESG count: 10\n\n\n5.3 - Vectorize the text\n\nvectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7)\n\n# Uses vectorization – takes all those words, getting into AI agent approach, rather than a single variable, converts variable into whole load of different dimensions\nX_counts = vectorizer.fit_transform(X_texts).toarray()\n\n\n5.4 - Apply tf-idf transformation\n\n# normalise the term frequency across the documents\ntfidfconverter = TfidfTransformer()\nX = tfidfconverter.fit_transform(X_counts).toarray()\n\n5.5 - Split data into train and test data and verify target variable data\n\n# spliut intotrain and test\nX_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.3, random_state=42, stratify=y_labels)\n\nprint(len(X_train))\nprint(len(y_train))\nprint(len(X_test))\nprint(len(y_test))\nprint(len(X_train)/(len(X_train)+len(X_test)))\nprint(len(X_test)/(len(X_train)+len(X_test)))\n\npd.Series(y_train).value_counts().plot(kind='bar', title='Count (Pre-SMOTE - Target Variable in y_train dataset)')\n\n30\n30\n13\n13\n0.6976744186046512\n0.3023255813953488\n\n\n\n\n\n\n\n\n\n5.6 - Create Models, perform cross validation and plot ROC AUC\n\n# perform cross validation and plot the output\nclassifiers = [\n   ('NB', GaussianNB()),\n   ('DT', DecisionTreeClassifier()),\n   ('RF', RandomForestClassifier()),\n   ('LR', LogisticRegression()),\n   ('NN', nn.MLPClassifier()),\n   ('XGB', XGBClassifier(n_estimators=100)),\n   ('SVC', SVC(probability=True))]\n\n# Logging setup\nlog_cols = [\"Classifier\", \"Accuracy (CV Mean)\"]\nlog = pd.DataFrame(columns=log_cols)\nprint(log)\n\n# Plot ROC curve for K-fold, cross-validated predictions\nplt.clf()\nplt.figure(figsize=(12, 8))\n\n# Define the cross-validation strategy, we tried different numbers 3, 5, 10, 20 and 30,can't exceed the number of samples. The data set is small so it doesn't take too long. Set at 3 for now to keep runtime low for demo purposes.\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nprint(cv)\n\n# Loop through classifiers \nfor name, model in classifiers:\n    name = model.__class__.__name__\n\n    # Get cross validation prediction and probability\n    y_pred_cv = cross_val_predict(model, X_train, y_train, cv=cv, method='predict')\n    y_pred_proba = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n\n    #  cross-validation accuracy scores and mean accuracy\n    cv_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n    cv_mean = np.mean(cv_scores)\n    print(f\"Cross-validation accuracy for {name}: {cv_mean}\") \n   \n    # ROC curve and AUC for cross-validated model\n    ## here we using cross_val_predict : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html\n\n    #sklearn.model_selection.cross_val_predict(estimator, X, y=None, *, groups=None, cv=None, n_jobs=None, verbose=0, \n    #params=None, pre_dispatch='2*n_jobs', method='predict')\n\n    ## basically getting model.predict_proba but for the cross validation figures. \n\n    #### Note we aren't comparing these directly, but just seeing if there are large differences between them more so. between cross validation \n    #### and the actual model evaluations below\n    ####### Note also this is all done on x_train and y_train. \n    \n    fpr_cv, tpr_cv, _ = roc_curve(y_train, y_pred_proba)\n    roc_auc_cv = auc(fpr_cv, tpr_cv)\n    print(f\"true pos and fal pos {fpr_cv} {tpr_cv}:\")\n\n    # Log results\n    log_entry = pd.DataFrame([[name, cv_mean]], columns=log_cols)\n    log = pd.concat([log, log_entry])\n   \n    # Plot ROC curves for cross-validation model\n    plt.plot(fpr_cv, tpr_cv, linestyle='--', label='%s CV ROC (area = %0.2f)' % (name, roc_auc_cv))\n\n    # Print confusion matrix for cross-validated predictions\n    cm_cv = confusion_matrix(y_train, y_pred_cv)\n    print(f\"Confusion Matrix for {name} (Cross-Validation):\")\n    print(cm_cv)\n    print('')\n\n\n# Finalize ROC plot\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Cross-Validation) - Pre-SMOTE Data - kfold nsplits = 5 ')\nplt.legend(loc=0, fontsize='small')\nplt.show()\n\n# Print log of results\nprint(log)\n\nEmpty DataFrame\nColumns: [Classifier, Accuracy (CV Mean)]\nIndex: []\nKFold(n_splits=5, random_state=42, shuffle=True)\nCross-validation accuracy for GaussianNB: 0.9333333333333333\ntrue pos and fal pos [0.         0.28571429 1.        ] [0. 1. 1.]:\nConfusion Matrix for GaussianNB (Cross-Validation):\n[[ 5  2]\n [ 0 23]]\n\nCross-validation accuracy for DecisionTreeClassifier: 0.7333333333333333\ntrue pos and fal pos [0.         0.85714286 1.        ] [0.         0.91304348 1.        ]:\nConfusion Matrix for DecisionTreeClassifier (Cross-Validation):\n[[ 2  5]\n [ 1 22]]\n\nCross-validation accuracy for RandomForestClassifier: 0.8333333333333334\ntrue pos and fal pos [0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.14285714 0.14285714 0.14285714\n 1.        ] [0.         0.04347826 0.13043478 0.2173913  0.30434783 0.47826087\n 0.60869565 0.69565217 0.73913043 0.7826087  0.91304348 1.\n 1.        ]:\nConfusion Matrix for RandomForestClassifier (Cross-Validation):\n[[ 1  6]\n [ 0 23]]\n\nCross-validation accuracy for LogisticRegression: 0.7666666666666666\ntrue pos and fal pos [0.         0.         0.         0.14285714 0.14285714 0.28571429\n 0.28571429 0.42857143 0.42857143 1.        ] [0.         0.04347826 0.7826087  0.7826087  0.86956522 0.86956522\n 0.95652174 0.95652174 1.         1.        ]:\nConfusion Matrix for LogisticRegression (Cross-Validation):\n[[ 0  7]\n [ 0 23]]\n\nCross-validation accuracy for MLPClassifier: 0.9666666666666668\ntrue pos and fal pos [0. 0. 0. 1.] [0.         0.04347826 1.         1.        ]:\nConfusion Matrix for MLPClassifier (Cross-Validation):\n[[ 6  1]\n [ 0 23]]\n\nCross-validation accuracy for XGBClassifier: 0.8\ntrue pos and fal pos [0.         0.         0.         0.         0.28571429 0.28571429\n 0.42857143 0.42857143 0.57142857 0.57142857 0.71428571 0.71428571\n 0.85714286 0.85714286 1.        ] [0.         0.04347826 0.13043478 0.30434783 0.47826087 0.69565217\n 0.69565217 0.7826087  0.7826087  0.82608696 0.82608696 0.95652174\n 0.95652174 1.         1.        ]:\nConfusion Matrix for XGBClassifier (Cross-Validation):\n[[ 1  6]\n [ 0 23]]\n\nCross-validation accuracy for SVC: 0.8333333333333334\ntrue pos and fal pos [0.         0.         0.         0.14285714 0.14285714 0.28571429\n 0.28571429 1.        ] [0.         0.04347826 0.82608696 0.82608696 0.95652174 0.95652174\n 1.         1.        ]:\nConfusion Matrix for SVC (Cross-Validation):\n[[ 2  5]\n [ 0 23]]\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n               Classifier  Accuracy (CV Mean)\n0              GaussianNB            0.933333\n0  DecisionTreeClassifier            0.733333\n0  RandomForestClassifier            0.833333\n0      LogisticRegression            0.766667\n0           MLPClassifier            0.966667\n0           XGBClassifier            0.800000\n0                     SVC            0.833333\n\n\n5.7 - Plot Accuracy data\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy (CV Mean)', y='Classifier', data=log, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy Normal Models')\nplt.show()\n\n\n\n\n\n\n\n\n5.8 - Apply SMOTE to data to oversample the minority class and verify target variable data\n\n# ---SMOTE----\n\n\n# Apply SMOTE only to training data\nsm = SMOTE(random_state=42)\nX_train_ps, y_train_ps = sm.fit_resample(X_train, y_train)\n\nprint('SMOTE over-sampling:')\nprint('-----')\nprint('length of \\'X_train\\' = {}'.format(len(X_train_ps)))\nprint('length of \\'y_train\\' = {}'.format(len(y_train_ps)))\nprint('-----')\n\n\n#plot post SMOTE of just the target variable\n# y_train_ps['Target'].value_counts().plot(kind='bar', title='Count (Post-SMOTE - Target Variable in y_train dataset')\n# y_train_ps.value_counts().plot(kind='bar', title='Count (Post-SMOTE - Target Variable in y_train dataset)')\npd.Series(y_train_ps).value_counts().plot(kind='bar', title='Count (Post-SMOTE - Target Variable in y_train dataset)')\n\n# re-associate the '_ps' naming with the standard X_train and y_train naming, as it it used through modelling process below\nX_train=X_train_ps\ny_train=y_train_ps\n\nSMOTE over-sampling:\n-----\nlength of 'X_train' = 46\nlength of 'y_train' = 46\n-----\n\n\n\n\n\n\n\n\n\n5.9 - Create the Models using SMOTE data set\n\n# perform cross validation and plot the output\nclassifiers = [\n   ('NB', GaussianNB()),\n   ('DT', DecisionTreeClassifier()),\n   ('RF', RandomForestClassifier()),\n   ('LR', LogisticRegression()),\n   ('NN', nn.MLPClassifier()),\n   ('XGB', XGBClassifier(n_estimators=100)),\n   ('SVC', SVC(probability=True))]\n\n# Logging setup\nlog_cols = [\"Classifier\", \"Accuracy (CV Mean)\"]\nlog = pd.DataFrame(columns=log_cols)\nprint(log)\n\n# Plot ROC curve for K-fold, cross-validated predictions\nplt.clf()\nplt.figure(figsize=(12, 8))\n\n# Define the cross-validation strategy, we tried different numbers 3, 5, 10, 20 and 30,can't exceed the number of samples. 20 appeears to be close ot the sweet spot for number of kfolds. The data set is small so it doesn't take too long. Set at 3 for now to keep runtime low for demo purposes.\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nprint(cv)\n\n# Loop through classifiers \nfor name, model in classifiers:\n    name = model.__class__.__name__\n\n    # Get cross validation prediction and probability\n    y_pred_cv = cross_val_predict(model, X_train, y_train, cv=cv, method='predict')\n    y_pred_proba = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n\n    #  cross-validation accuracy scores and mean accuracy\n    cv_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n    cv_mean = np.mean(cv_scores)\n    print(f\"Cross-validation accuracy for {name}: {cv_mean}\") \n   \n    # ROC curve and AUC for cross-validated model\n    ## here we using cross_val_predict : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html\n\n    #sklearn.model_selection.cross_val_predict(estimator, X, y=None, *, groups=None, cv=None, n_jobs=None, verbose=0, \n    #params=None, pre_dispatch='2*n_jobs', method='predict')\n\n    ## basically getting model.predict_proba but for the cross validation figures. \n\n    #### Note we aren't comparing these directly, but just seeing if there are large differences between them more so. between cross validation \n    #### and the actual model evaluations below\n    ####### Note also this is all done on x_train and y_train. \n    \n    fpr_cv, tpr_cv, _ = roc_curve(y_train, y_pred_proba)\n    roc_auc_cv = auc(fpr_cv, tpr_cv)\n    print(f\"true pos and fal pos {fpr_cv} {tpr_cv}:\")\n\n    # Log results\n    log_entry = pd.DataFrame([[name, cv_mean]], columns=log_cols)\n    log = pd.concat([log, log_entry])\n   \n    # Plot ROC curves for cross-validation model\n    plt.plot(fpr_cv, tpr_cv, linestyle='--', label='%s CV ROC (area = %0.2f)' % (name, roc_auc_cv))\n\n    # Print confusion matrix for cross-validated predictions\n    cm_cv = confusion_matrix(y_train, y_pred_cv)\n    print(f\"Confusion Matrix for {name} (Cross-Validation):\")\n    print(cm_cv)\n    print('')\n\n\n# Finalize ROC plot\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Cross-Validation) - Post SMOTE Data - kfold nsplits = 5 ')\nplt.legend(loc=0, fontsize='small')\nplt.show()\n\n# Print log of results\nprint(log)\n\nEmpty DataFrame\nColumns: [Classifier, Accuracy (CV Mean)]\nIndex: []\nKFold(n_splits=5, random_state=42, shuffle=True)\nCross-validation accuracy for GaussianNB: 1.0\ntrue pos and fal pos [0. 0. 1.] [0. 1. 1.]:\nConfusion Matrix for GaussianNB (Cross-Validation):\n[[23  0]\n [ 0 23]]\n\nCross-validation accuracy for DecisionTreeClassifier: 0.8022222222222222\ntrue pos and fal pos [0.         0.04347826 1.        ] [0.         0.69565217 1.        ]:\nConfusion Matrix for DecisionTreeClassifier (Cross-Validation):\n[[22  1]\n [ 6 17]]\n\nCross-validation accuracy for RandomForestClassifier: 1.0\ntrue pos and fal pos [0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.26086957 0.34782609 0.43478261\n 0.60869565 0.73913043 0.91304348 1.        ] [0.         0.04347826 0.13043478 0.2173913  0.56521739 0.65217391\n 0.69565217 0.7826087  1.         1.         1.         1.\n 1.         1.         1.         1.        ]:\nConfusion Matrix for RandomForestClassifier (Cross-Validation):\n[[23  0]\n [ 0 23]]\n\nCross-validation accuracy for LogisticRegression: 1.0\ntrue pos and fal pos [0. 0. 0. 1.] [0.         0.04347826 1.         1.        ]:\nConfusion Matrix for LogisticRegression (Cross-Validation):\n[[23  0]\n [ 0 23]]\n\nCross-validation accuracy for MLPClassifier: 0.9800000000000001\ntrue pos and fal pos [0. 0. 0. 1.] [0.         0.04347826 1.         1.        ]:\nConfusion Matrix for MLPClassifier (Cross-Validation):\n[[23  0]\n [ 1 22]]\n\nCross-validation accuracy for XGBClassifier: 0.8688888888888888\ntrue pos and fal pos [0.         0.         0.         0.         0.         0.\n 0.         0.04347826 0.04347826 0.08695652 0.08695652 0.13043478\n 0.13043478 0.17391304 0.26086957 0.43478261 0.65217391 0.7826087\n 0.82608696 0.91304348 1.        ] [0.         0.04347826 0.08695652 0.34782609 0.39130435 0.56521739\n 0.60869565 0.60869565 0.73913043 0.73913043 0.7826087  0.7826087\n 0.91304348 0.91304348 0.91304348 0.91304348 1.         1.\n 1.         1.         1.        ]:\nConfusion Matrix for XGBClassifier (Cross-Validation):\n[[20  3]\n [ 3 20]]\n\nCross-validation accuracy for SVC: 1.0\ntrue pos and fal pos [0. 0. 0. 1.] [0.         0.04347826 1.         1.        ]:\nConfusion Matrix for SVC (Cross-Validation):\n[[23  0]\n [ 0 23]]\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n               Classifier  Accuracy (CV Mean)\n0              GaussianNB            1.000000\n0  DecisionTreeClassifier            0.802222\n0  RandomForestClassifier            1.000000\n0      LogisticRegression            1.000000\n0           MLPClassifier            0.980000\n0           XGBClassifier            0.868889\n0                     SVC            1.000000\n\n\n5.10 - Plot Accuracy data\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy (CV Mean)', y='Classifier', data=log, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy Normal Models')\nplt.show()\n\n\n\n\n\n\n\n\n5.10 - Evaluate Model Performance - Some models performed reasonably well/ Was not expecting this given the limited data size. - MLPClassifier gives 1.0 ROC curve value and an accuracy of .925 in both pre and post SMOTE Data. This rings alarms bells that data is overfitting, or something is not right. This would require more investigation, would discount this model data. - Best performing models - LogisticRegression - largest ROC AUC value at .95 but has an accuracy of .75 which is 5th best accuracy. Performed well. - XGBClassifier - 0.81 ROC AUC value abut has an accuracy of 0.875. Again performed well. - Other also performed reasonably well.\n\nLogisticRegression had the followinfg confusion matrix.\n\n[[ 0  7]\n [ 0 23]]\n\n\n=&gt; 7 False negatives - said 'bad' esg but was 'good' esg company\n=&gt; 23 True Positives - said 'good' esg and was 'good' esg company\n=&gt; 4 True Negatives - said 'bad' esg and was 'bad' esg company\n=&gt; 0 False Positive - said 'good' esg and was 'bad' esg company\n\nXGBClassifier had the following confusion matrix\n\n[[ 4  3]\n [ 1 22]]\n\n=&gt; 3 False negatives - said 'bad' esg but was 'good' esg company\n=&gt; 22 True Positives - said 'good' esg and was 'good' esg company\n=&gt; 4 True Negatives - said 'bad' esg and was 'bad' esg company\n=&gt; 0 False Positive - said 'good' esg and was 'bad' esg company\nFewer False negatives is important as means telling people the company has poor ethical esg values when infact it has good esg values. This could potentially be unfair and lost business to the companies who were incorrectly labelled. This would need further investigation to try minimise the False negative values.\nPost-SMOTE applied data did make a difference. SVC has an ROC Value of .99, which again rings alarm bells and means this along with MLPClassifier would both be discounted as there is an issue with these being 1.0 or close to 1.0.\nXGBClassifier and LogisticRegression remained stable and similar to the use of pre-smote data, whereas DecisionTree and RandomForest both deteriorated with SMOTE data.\nBest model overall would possibly be a combination of LogicalRegression and XGBClassifier. These both performed well and were stable in both scenarios.\nOverall, a larger data set is still required to validate the data and performance and further investigation and validation is required.\n\nStep 6 - Conclusions and Learnings\nCan we predict whether a company has good or bad esg practices based on reviewing their company reports. This is indicated by\n\n1 = good ‘esg’ company\n0 = bad ‘esg’ company\n\nSomewhat surprisingly, two elements wold lead us to believe that this shows promise in text mining analysis and classification as a method to indicate ‘esg’ company performance 1. Part 1. Data Analysis - The graph Positive Ethical Keyword Count per Company per 1000 words matches our expectations. - Least Ethical with Lowest Counts per 1000 words = exxonmobil, jbs and aramco - These 3 were selected as they have very low esg rating across our esg ratings i.e. bad esg companies - Companies with the Most ethical word counts per 1000 words = kerrygroup, crh, snmufitkappa and kingspan - These 4 were selected due to their high esg ratings i.e. esg good esg companies 1. Part 2. Classification - The cross fold validation and smote correctly correctly predicted 10 and 5 bad companies respectively. This is tghe most positive sign for me and a good indication of potential, especially given our small data set. true negatives woudl be hardest to predict as companies will use positive terms in reports many times, but the model cna still predict bad esg performers despite this.\n\n6.1 Challenges\n\nStandardizing and cleaning the date. Not all reports are the same or standardized. Lots of variations in data provided and formats. Volume of data. Need to get a lot more company report\nTime to process the pdfs is approx between 30 secs and 2 mins per pdf. This adds up when dealing with large numbers of pdfs… It takes approx 40 mins to process our small data set pof pdfs\nCompanies can stack the reports by repeating positive terms. Always a risk, even if our classification models could see through that in a good few instances in our data set\nStorage - we need to increase our dataset hugely across a traneg of sources to improve accuracy. Storage may become an issue as data storage needs increase….\n\nwe used 44 files across 7 companies which took up aprox 500 MB of space\n\n\n\n\n6.2 Improvements\n\ntie it into Company Registration Office (CRO) data to include ownership details\nGet more sample data\nInclude social media and news articles for each company to enhance the data and catch potentisal negativity around it\nInclude court documetn data\nif have different data sources i.e.company reports, social media, news, courts data can weight those accordingly to give more accurate result.\n\nOverall, this has been surprisingly successful. The initial assumption would be that it would be too difficult to discern a companies esg status solely base don their company reports. However, this shows great potential if the data set could be increased and especially if it can eb broadened to include other sources other than self authored reports.\n\n\n\nAppendix - References\n\noralytics &gt;&gt;&gt; #GE2020 Analysing Party Manifestos using Python\nhttps://github.com/pdfminer/pdfminer.six &gt;&gt;&gt; community maintained fork of the original PDFMiner\npdfprimer docs\nhttps://www.nltk.org/\nhttps://docs.python.org/3/howto/regex.html\nhttps://regex101.com/\nGeneral ESG references outside of those already included are :\n\nhttps://www.responsible-investor.com/\nhttps://www.knowesg.com/\nvarious news sites\n\n\n[2] Python – Convert list of dictionaries to JSON"
  },
  {
    "objectID": "dataanalysis.html",
    "href": "dataanalysis.html",
    "title": "dataanalysis",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "dataanalysis.html#quarto",
    "href": "dataanalysis.html#quarto",
    "title": "dataanalysis",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "notebooks/exoplanet_id_clustering_final.html#problem-statement",
    "href": "notebooks/exoplanet_id_clustering_final.html#problem-statement",
    "title": "TU257 - Assignment B",
    "section": "Problem Statement",
    "text": "Problem Statement\nAs planets in far away solar systems roate about suns, the light the reach earth from those sun is reduced, as the planet passes across its path. Monitoring the light coming from stars and looking for those dips in light intensity/brightness can be a way of identifying far away planets orbiting suns.\nNasa have programs actively looking to identify these ‘exoplanets’ with the TESS and Kepler programs. They make this data availabel to the public and it is also available through some universities.\n\nhttps://science.nasa.gov/exoplanets/\nhttps://exoplanets.nasa.gov/tess/\n\nQuestion: can we identify any similarities in light data from multiple stars which may lead to identification of planet passing in front of a star.\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport requests\n\n\nfrom lightkurve import search_lightcurvefile, LightCurveFile, read #[1]\nfrom ydata_profiling import ProfileReport\nimport sweetviz as sv\n\n#load the libraries we need for this example\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import MinMaxScaler\nfrom matplotlib import pyplot as plt\nfrom scipy.stats.mstats import winsorize\nfrom scipy.signal import find_peaks, peak_widths\nimport astropy.units as u # [4]\nfrom astropy.timeseries import BoxLeastSquares\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.cm as cm\n\n\n%matplotlib inline\n\n\nFile Structure:\n\nStep 1 - Identifing and Downloading Data\n\n1.1 Manually download .sh file\n1.2 Download Data files locally\n1.4 Put Data into Dataframe\n\nStep 2 - Data Eploration\n\n2.1 General Overview of state of data i.e. shape, missing values, duplicates, unusual characters etc\n2.2 Generate report for analysis\n2.3 Initial Data Insights\n2.4 Remove unneeded columns and verify data as expected after deletion\n2.5 Further Data Insights\n2.6 Plot for how the data looks for different star types\n2.7 Plots of distribution of data per column\n2.8 Plots of outliers in the column data\n2.9 Further Data Insights from plots\n\nStep 3 - Clean Data\n\n3.1 Drop empty columns from ‘pdcsap_flux’ and verify successful\n3.2 Further Data insight\n3.3 Normalise the ‘pdcsap_sap_flux’ data and verify the chnages in a plot\n3.4 Winsorise selected numerical columns and verify the chnages in a plot\n\nStep 4 - Extract a feature from the Data to cluster\n\n4.1 Create new df of latest to work in\n4.2 Determine light curve dip data values from ‘pdcsap_flux_norm’ and Plot them to verify data is as expect\n4.3 Calculate and Add flux dip data, ‘num_flux_dips’ to new column in dataframe\n4.4 Calculate the variability of the flux per star, we’ll use the standard deviation to represent this, ‘flux_std’\n4.5 Calculate average dip duration per star, ‘dip_duration’\n4.6 Merge all calculated per star data into a dataframe\n4.7 Data Insights\n4.8 Normalise the dataframe and verify it is as we expect\n\nStep 5 - Clustering\n\n5.1 - Create a new dataframe with one row per star on our feature\n5.2 Plot elbow curve to determine appropriate cluster count\n5.3 Create KMeans cluster and analyze data\n5.4 Plot Kmeans cluster data\n5.5 Plot Kmeans cluster data in 3D\n5.6 Initial Data Insights\n5.7 Plot Kmeans++ data\n5.8 Calculate and Display the Silhouette values\n5.9 Alternative silhouette data plot\n5.10 Data Insights from silhouette plots\n\nStep 6 - Conclusions and Learning\n\n6.1 Challenges\n6.2 Improvements\n\nAppendix - References"
  },
  {
    "objectID": "notebooks/exoplanet_id_clustering_final.html#step-1---sourcing-and-downloding-data",
    "href": "notebooks/exoplanet_id_clustering_final.html#step-1---sourcing-and-downloding-data",
    "title": "TU257 - Assignment B",
    "section": "Step 1 - Sourcing and Downloding Data",
    "text": "Step 1 - Sourcing and Downloding Data\nSpace Telescope Sciende Institue\nDownload all TESS Full Frame Image (FFI), target pixel (TP) files, light curve (LC) files, or data validation (DV) files by sector using the cURL scripts provided. Digital Object Identifiers (DOIs) for each data product type in each TESS Sector are provided at below url\nI downloaded 4 or 5 of these from lightkurve section on the below page and ended up using sector 89. LightKurve contains actial light kurve data, and sector 89 just as it was at the top of the list\nhttps://archive.stsci.edu/tess/bulk_downloads/bulk_downloads_ffi-tp-lc-dv.html\ndownload data from here &gt;&gt; https://exoplanetarchive.ipac.caltech.edu/\nThis is .sh file which and below is a sample of its content data is stored in FIT files which is s standard for astronomical data this sh file contains a list of 12K plus fit files and about 2 to 2.5 Mbs each… which is too many for our project. we’ll just take a subset\n#!/bin/sh\ncurl -C - -L -o tess2025042113628-s0089-0000000000872755-0286-s_lc.fits https://mast.stsci.edu/api/v0.1/Download/file/?uri=mast:TESS/product/tess2025042113628-s0089-0000000000872755-0286-s_lc.fits\ncurl -C - -L -o tess2025042113628-s0089-0000000000991159-0286-s_lc.fits https://mast.stsci.edu/api/v0.1/Download/file/?uri=mast:TESS/product/tess2025042113628-s0089-0000000000991159-0286-s_lc.fits\ncurl -C - -L -o tess2025042113628-s0089-0000000000990995-0286-s_lc.fits https://mast.stsci.edu/api/v0.1/Download/file/?uri=mast:TESS/product/tess2025042113628-s0089-0000000000990995-0286-s_lc.fits\ncurl -C - -L -o tess2025042113628-s0089-0000000001228450-0286-s_lc.fits https://mast.stsci.edu/api/v0.1/Download/file/?uri=mast:TESS/product/tess2025042113628-s0089-0000000001228450-0286-s_lc.fits\ncurl -C - -L -o tess2025042113628-s0089-0000000001137183-0286-s_lc.fits https://mast.stsci.edu/api/v0.1/Download/file/?uri=mast:TESS/product/tess2025042113628-s0089-0000000001137183-0286-s_lc.fits\n chmod +x tesscurl_sector_89_lc.sh ./tesscurl_sector_89_lc.sh\nthere is some data available on Kaggle &gt;&gt; https://www.kaggle.com/datasets/vijayveersingh/kepler-and-tess-exoplanet-data however we took directly form source and decided to explore and clean it ourselves\ndownload data from here &gt;&gt; https://exoplanetarchive.ipac.caltech.edu/\nNeeded more control over whatwe download due to the volume of files. Crated the below python script to download the files we neede\n1.1 Manually download .sh file\nFirst step is to manually download ‘esscurl_sector_89_lc.sh’ file from the following URL from the Lightcurve section.\nMikulski ASrchive for ace Telescopes\nPlace it in your local folder alongside this notebook\nThe shell file contains a list individual data files. Sector relates to different parts of the sky which are observed.\n1.2 Download Data files locally\n\n# define paths\nworking_path = os.getcwd()\nfit_file_storage_path = os.path.join(working_path, \"data\")\nprint(f\"Notebook is running from here &gt;&gt; {working_path}\")\nprint(f\"Data is located here &gt;&gt;  {working_path}\")\n\nNotebook is running from here &gt;&gt; d:\\GG\\data_anal_cmpu4077\\labs\\assignment-b\\working\\clustering\\nasa\\working\\working\\working\nData is located here &gt;&gt;  d:\\GG\\data_anal_cmpu4077\\labs\\assignment-b\\working\\clustering\\nasa\\working\\working\\working\n\n\n\n# Python cel to identify data files to download form the .sh file and to then download them locally\n# need to manually add .sh filename that we downloaded\n# where to place the files and where the shell script is located#\n# Time - 100 fit files take approx 3 mins to download\n\nsh_file_path = os.path.join(working_path, 'tesscurl_sector_89_lc.sh')\n\n\n#create empty list of files we will download and the url to get them\nfiles_to_download = []\n# check if download directory exists\nos.makedirs(fit_file_storage_path, exist_ok=True)\n\n# open the .sh file and get the filename and url values from it and place in files_to_download list\nwith open(sh_file_path, \"r\") as f:\n    for line in f:\n        # strip the elements of the line into \n        if line.startswith(\"curl\") and \"-o\" in line:\n            #remove leading and trailing whitespace and split into list of strings with spaces as seperatopr\n            parts = line.strip().split()\n            # include in try-except to not fail on any errors\n            try:\n                # the download url is the last part of the line and the filename is after '-o'\n                filename = parts[parts.index(\"-o\") + 1]\n                # the URL is the last item in the line\n                # url is the last element of the line\n                url = parts[-1]\n                # append the filename and url to files_to_download list\n                if url.startswith(\"http\"):\n                    files_to_download.append((filename, url))\n            except (ValueError, IndexError):\n                continue\n\n#define how many files to download\nfiles_to_download = files_to_download[:100]\n \nfor filename, url in files_to_download:\n    # cdefine wherre to put the files\n    download_path_and_filename = os.path.join(fit_file_storage_path, filename)\n    # if have file already skip it\n    if os.path.exists(download_path_and_filename):\n        print(f\"Already have that file - skipping: {filename}\")\n        continue\n    print(f\"....Downloading:... {filename}\")\n    # download the file, chunking the download appears ot be best practice with large files\n    response = requests.get(url, stream=True)\n    if response.status_code == 200:\n        with open(download_path_and_filename, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n        # print(f\" Downloaded ...\")\n    else:\n        print(f\" Failed to download {filename} (status code: {response.status_code})\")\n        \n        \n# [4] [Request and Response Objects](https://docs.python-requests.org/en/latest/user/advanced/)\n# [5] [Download large file in python with requests](https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests)\n\nAlready have that file - skipping: tess2025042113628-s0089-0000000000872755-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000000991159-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000000990995-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001228450-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001137183-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001008761-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001008931-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000000990903-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000000991098-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000000874892-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000000991806-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001191321-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000000872155-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000000800254-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001008989-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001009252-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001209680-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001203886-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000000992137-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001141092-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001229682-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001228170-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001013471-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001018843-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001230105-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001258971-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001261860-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001268462-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001338828-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001339767-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001340317-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001342846-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001344656-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001359495-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001362343-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001360104-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001399933-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001401516-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001402874-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001404122-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001442449-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001446147-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001446262-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001474831-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001503583-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001504460-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001511418-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001524976-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001523352-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001525480-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001530056-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001531533-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001564504-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001622841-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001573395-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001625424-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001680927-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001627306-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001712781-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001712865-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001730647-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001731678-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001738861-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001732081-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001740238-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001739824-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001818589-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001742152-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001861139-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001861247-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001969270-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001870180-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001969495-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001969785-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000001973623-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002020365-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002020964-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002084703-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002132354-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002094712-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002137137-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002140167-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002221045-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002220886-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002224747-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002222928-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002225329-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002224750-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002241706-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002246427-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002251147-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002350053-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000002397800-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000003054883-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000003127674-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000003130636-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000003227721-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000003356565-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000003516034-0286-s_lc.fits\nAlready have that file - skipping: tess2025042113628-s0089-0000000003448013-0286-s_lc.fits\n\n\n1.3 - Put Data into Dataframe\n\n#  have 100 fits files &gt;&gt; tess2025042113628-s0089-0000000024918650-0286-s_lc.fits\n# merge all 100 into single dataframe\n\n#list to store data per file\ndf_list=[]\n\nfor i, filename in enumerate(os.listdir(fit_file_storage_path)):\n    if filename.endswith('.fits') and i &lt; 100:\n        filepath = os.path.join(fit_file_storage_path, filename)\n        try:\n            lc = read(filepath)  # ← No .normalize(), no .remove_nans()\n            df = lc.to_pandas()\n            # need to explicitly call 'time' otherwise end up with a converted value of it...\n            # this is how the learnKurve package works\n            df['time'] = lc.time.value\n            # adding a column for source file so we cnatrack it as each source file represents a different star and can then filter by star\n            df['source_file'] = filename\n            df_list.append(df)\n        except Exception as e:\n            print(f\" Skipping {filename} due to error: {e}\")\n\nraw_df = pd.concat(df_list, ignore_index=True)"
  },
  {
    "objectID": "notebooks/exoplanet_id_clustering_final.html#step-2---data-exploration",
    "href": "notebooks/exoplanet_id_clustering_final.html#step-2---data-exploration",
    "title": "TU257 - Assignment B",
    "section": "Step 2 - Data Exploration",
    "text": "Step 2 - Data Exploration\nWill perform some general data exploration to get a feel for the data\n2.1 - General Overview of state of data i.e. shape, missing values, duplicates, unusual characters, objecttypes etc\n\n# quick look at data to see if its clean, no duplicates, null values unusual characters etc\n# convert it to d first so can easily run some checks over it\n# view various dataframe details\n# checking for null values and specific unknown values\nprint (\"Rows     : \" , raw_df.shape[0])\nprint (\"Columns  : \" , raw_df.shape[1])\nprint('------')\nprint (\"\\nFeatures : \\n\" , raw_df.columns.tolist())\nprint('------')\nprint (\"\\nMissing values :  \", raw_df.isnull().sum().values.sum())\nprint('------')\nprint (\"\\nMissing values per column:  \\n\", raw_df.isnull().sum())\nprint('------')\nprint('\\nnum of dups in df   =   {}'.format(raw_df.duplicated().sum()))\nprint('------')\nprint (\"\\nUnique values :  \\n\", raw_df.nunique())\nprint('------')\nprint('\\n null values = ', raw_df.isnull().values.any())\nprint('------')\nraw_df_missing = (df=='?').sum()\nprint('\\\"  ?   \\\" values present =  \\n',raw_df_missing)\nprint('------')\nraw_df_unknown = (raw_df=='unknown').sum()\nprint('\\\"   unknown   \\\" values present =  \\n',raw_df_unknown)\nprint('------')\nraw_df_na = raw_df.isin(['N/A', 'N\\\\A', 'NA', 'n/a', 'n\\\\A', 'na', 'N_A']).sum()\nprint('    NA     values present = \\n', raw_df_na)\nprint('------')\nraw_df_none = raw_df.isin(['None', 'none']).sum()\nprint('    none    values present = \\n', raw_df_none)\n\nRows     :  2025700\nColumns  :  25\n------\n\nFeatures : \n ['flux', 'flux_err', 'timecorr', 'cadenceno', 'centroid_col', 'centroid_row', 'sap_flux', 'sap_flux_err', 'sap_bkg', 'sap_bkg_err', 'pdcsap_flux', 'pdcsap_flux_err', 'quality', 'psf_centr1', 'psf_centr1_err', 'psf_centr2', 'psf_centr2_err', 'mom_centr1', 'mom_centr1_err', 'mom_centr2', 'mom_centr2_err', 'pos_corr1', 'pos_corr2', 'time', 'source_file']\n------\n\nMissing values :   5170008\n------\n\nMissing values per column:  \n flux                 41029\nflux_err             41029\ntimecorr                 0\ncadenceno                0\ncentroid_col         22042\ncentroid_row         22042\nsap_flux             21905\nsap_flux_err         21905\nsap_bkg              21905\nsap_bkg_err          21905\npdcsap_flux          41029\npdcsap_flux_err      41029\nquality                  0\npsf_centr1         1182889\npsf_centr1_err     1182889\npsf_centr2         1182889\npsf_centr2_err     1182889\nmom_centr1           22042\nmom_centr1_err       22042\nmom_centr2           22042\nmom_centr2_err       22042\npos_corr1            27232\npos_corr2            27232\ntime                     0\nsource_file              0\ndtype: int64\n------\n\nnum of dups in df   =   0\n------\n\nUnique values :  \n flux               1489542\nflux_err           1549840\ntimecorr           1558093\ncadenceno            20257\ncentroid_col       2003656\ncentroid_row       2003658\nsap_flux           1628077\nsap_flux_err       1595392\nsap_bkg            1873620\nsap_bkg_err        1791374\npdcsap_flux        1489542\npdcsap_flux_err    1549840\nquality                  7\npsf_centr1          842811\npsf_centr1_err      785667\npsf_centr2          842785\npsf_centr2_err      767886\nmom_centr1         2003656\nmom_centr1_err     1768460\nmom_centr2         2003658\nmom_centr2_err     1778454\npos_corr1          1969264\npos_corr2          1973227\ntime               2025554\nsource_file            100\ndtype: int64\n------\n\n null values =  True\n------\n\"  ?   \" values present =  \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\ncentroid_col       0\ncentroid_row       0\nsap_flux           0\nsap_flux_err       0\nsap_bkg            0\nsap_bkg_err        0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\npsf_centr1         0\npsf_centr1_err     0\npsf_centr2         0\npsf_centr2_err     0\nmom_centr1         0\nmom_centr1_err     0\nmom_centr2         0\nmom_centr2_err     0\npos_corr1          0\npos_corr2          0\ntime               0\nsource_file        0\ndtype: int64\n------\n\"   unknown   \" values present =  \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\ncentroid_col       0\ncentroid_row       0\nsap_flux           0\nsap_flux_err       0\nsap_bkg            0\nsap_bkg_err        0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\npsf_centr1         0\npsf_centr1_err     0\npsf_centr2         0\npsf_centr2_err     0\nmom_centr1         0\nmom_centr1_err     0\nmom_centr2         0\nmom_centr2_err     0\npos_corr1          0\npos_corr2          0\ntime               0\nsource_file        0\ndtype: int64\n------\n    NA     values present = \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\ncentroid_col       0\ncentroid_row       0\nsap_flux           0\nsap_flux_err       0\nsap_bkg            0\nsap_bkg_err        0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\npsf_centr1         0\npsf_centr1_err     0\npsf_centr2         0\npsf_centr2_err     0\nmom_centr1         0\nmom_centr1_err     0\nmom_centr2         0\nmom_centr2_err     0\npos_corr1          0\npos_corr2          0\ntime               0\nsource_file        0\ndtype: int64\n------\n    none    values present = \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\ncentroid_col       0\ncentroid_row       0\nsap_flux           0\nsap_flux_err       0\nsap_bkg            0\nsap_bkg_err        0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\npsf_centr1         0\npsf_centr1_err     0\npsf_centr2         0\npsf_centr2_err     0\nmom_centr1         0\nmom_centr1_err     0\nmom_centr2         0\nmom_centr2_err     0\npos_corr1          0\npos_corr2          0\ntime               0\nsource_file        0\ndtype: int64\n\n\n\nraw_df.head(5)\n\n\n\n\n\n\n\n\n\nflux\nflux_err\ntimecorr\ncadenceno\ncentroid_col\ncentroid_row\nsap_flux\nsap_flux_err\nsap_bkg\nsap_bkg_err\n...\npsf_centr2\npsf_centr2_err\nmom_centr1\nmom_centr1_err\nmom_centr2\nmom_centr2_err\npos_corr1\npos_corr2\ntime\nsource_file\n\n\n\n\n0\n89815.570312\n34.665829\n0.004045\n1793292\n302.427308\n1722.937178\n86317.460938\n32.711414\n2879.953613\n7.686441\n...\n1722.929850\n0.000325\n302.427308\n0.000343\n1722.937178\n0.000322\n0.039176\n0.013342\n3718.142699\ntess2025042113628-s0089-0000000000872755-0286-...\n\n\n1\n89780.367188\n34.662632\n0.004045\n1793293\n302.424837\n1722.931474\n86265.046875\n32.708397\n2907.574463\n7.705001\n...\n1722.924192\n0.000326\n302.424837\n0.000343\n1722.931474\n0.000322\n0.036725\n0.007862\n3718.144088\ntess2025042113628-s0089-0000000000872755-0286-...\n\n\n2\n89767.437500\n34.662403\n0.004045\n1793294\n302.428540\n1722.932351\n86262.656250\n32.708183\n2908.053467\n7.705613\n...\n1722.924750\n0.000326\n302.428540\n0.000343\n1722.932351\n0.000322\n0.040231\n0.008568\n3718.145477\ntess2025042113628-s0089-0000000000872755-0286-...\n\n\n3\n89746.093750\n34.659431\n0.004045\n1793295\n302.428077\n1722.941915\n86209.960938\n32.705379\n2909.015381\n7.712942\n...\n1722.935068\n0.000324\n302.428077\n0.000343\n1722.941915\n0.000322\n0.040026\n0.018598\n3718.146866\ntess2025042113628-s0089-0000000000872755-0286-...\n\n\n4\n89784.898438\n34.648308\n0.004045\n1793296\n302.428227\n1722.930322\n86214.679688\n32.694878\n2902.173340\n7.696274\n...\n1722.922422\n0.000328\n302.428227\n0.000343\n1722.930322\n0.000323\n0.039194\n0.006230\n3718.148255\ntess2025042113628-s0089-0000000000872755-0286-...\n\n\n\n\n5 rows × 25 columns\n\n\n\n2.2 Generate sweetviz report for analysis\n\n# ydata profiling report takes approx 30 mins + to generate compared top ~ 7/8 mins for sweetviz\n# created profiling report and analysed data using it. however given time toi run, have removed from notebook\n# and only generating sweetviz report here, which was also used for analysis\n# craete sweetviz report\nsweet_rep=sv.analyze(raw_df)\nsweet_rep.show_html('report-sweet-3333.html')\n\n\n\n\nReport report-sweet-3333.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n\n\n\nraw_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2025700 entries, 0 to 2025699\nData columns (total 25 columns):\n #   Column           Dtype  \n---  ------           -----  \n 0   flux             float32\n 1   flux_err         float32\n 2   timecorr         float32\n 3   cadenceno        int32  \n 4   centroid_col     float64\n 5   centroid_row     float64\n 6   sap_flux         float32\n 7   sap_flux_err     float32\n 8   sap_bkg          float32\n 9   sap_bkg_err      float32\n 10  pdcsap_flux      float32\n 11  pdcsap_flux_err  float32\n 12  quality          int32  \n 13  psf_centr1       float64\n 14  psf_centr1_err   float32\n 15  psf_centr2       float64\n 16  psf_centr2_err   float32\n 17  mom_centr1       float64\n 18  mom_centr1_err   float32\n 19  mom_centr2       float64\n 20  mom_centr2_err   float32\n 21  pos_corr1        float32\n 22  pos_corr2        float32\n 23  time             float64\n 24  source_file      object \ndtypes: float32(15), float64(7), int32(2), object(1)\nmemory usage: 255.0+ MB\n\n\n2.3 - Initial Data Insights\nstandard deviation high for flus and pdsc_flux values &gt; 2168736.75 AND 2168736.75\n\nkeep columns -\n\nanything that includes the phrase’flux’ is related to light intensity/brightness and will kept\n‘flux’ - intensity / brightness of the star light - this is raw data\n‘flux_err’ - error associated wuth flux\n‘sap_flux’ - - related to intensity / brightness of star light in aperture in electrons p sec. Has not been co-trended\n‘pdcsap_flux’ - related to intensity / brightness of star light in aperture in electrons p sec. Has been co-trended. This is flux data which has been cleaned. It is a good candidate to cluster and will be our main initial focus.\n‘time’ - time value recorded in BJDays Barymetric-a standard for astronomy\n‘timecorr’ - an incremental time value, not related back to real time I don’t think\n‘source_file’ - can group for individual stats using this, as such will keep\n\nreomving the following columns as they will not add value to our clustering\n\n“cadenceno” &gt; like an index &gt; ‘a unique integer that is incremented with each cadence’ “psf_centr1”, “psf_centr1_err”, “psf_centr2”, “psf_centr2_err” &gt;&gt; Pixel Response Funtion (PSF) - think related to calibration of pixels in telescope. Not relevant to us in clustering.\n“mom_centr1”, “mom_centr1_err”, “mom_centr2”, “mom_centr2_err” &gt;&gt; moment-derived column - related to location of target position at cadences\n“centroid_col”, “centroid_row” &gt;&gt; related to locations of targets and centroid and howp they are identified, tracked\n“pos_corr1”, “pos_corr2” &gt;&gt; related to local image motion in relation to pixels\n“sap_bkg”, “sap_bkg_err” &gt;&gt; background flux over aperture for each pixel\n\n\n[2]\n2.4 Remove unneeded columns and verify data as expected after deletion\n\n# remove columns which we will not use in cluatering\ncolumns_to_drop = [\n    \"psf_centr1\", \"psf_centr1_err\", \"psf_centr2\", \"psf_centr2_err\",\n    \"mom_centr1\", \"mom_centr1_err\", \"mom_centr2\", \"mom_centr2_err\",\n    \"centroid_col\", \"centroid_row\", \"pos_corr1\", \"pos_corr2\", \"sap_bkg\", \"sap_bkg_err\"\n]\n\nclean_df = raw_df.drop(columns=columns_to_drop)\n\n\n# quick look at data to see if its as we expectafter removal of columns\n\n# view various dataframe details\n# checking for null values and specific unknown values\nprint (\"Rows     : \" , clean_df.shape[0])\nprint (\"Columns  : \" , clean_df.shape[1])\nprint('------')\nprint (\"\\nFeatures : \\n\" , clean_df.columns.tolist())\nprint('------')\nprint (\"\\nMissing values :  \", clean_df.isnull().sum().values.sum())\nprint('------')\nprint (\"\\nMissing values per column:  \\n\", clean_df.isnull().sum())\nprint('------')\nprint('\\nnum of dups in df   =   {}'.format(clean_df.duplicated().sum()))\nprint('------')\nprint (\"\\nUnique values :  \\n\", clean_df.nunique())\nprint('------')\nprint('\\n null values = ', clean_df.isnull().values.any())\nprint('------')\nclean_df_missing = (df=='?').sum()\nprint('\\\"  ?   \\\" values present =  \\n',clean_df_missing)\nprint('------')\nclean_df_unknown = (clean_df=='unknown').sum()\nprint('\\\"   unknown   \\\" values present =  \\n',clean_df_unknown)\nprint('------')\nclean_df_na = clean_df.isin(['N/A', 'N\\\\A', 'NA', 'n/a', 'n\\\\A', 'na', 'N_A']).sum()\nprint('    NA     values present = \\n', clean_df_na)\nprint('------')\nclean_df_none = clean_df.isin(['None', 'none']).sum()\nprint('    none    values present = \\n', clean_df_none)\n\nRows     :  2025700\nColumns  :  11\n------\n\nFeatures : \n ['flux', 'flux_err', 'timecorr', 'cadenceno', 'sap_flux', 'sap_flux_err', 'pdcsap_flux', 'pdcsap_flux_err', 'quality', 'time', 'source_file']\n------\n\nMissing values :   207926\n------\n\nMissing values per column:  \n flux               41029\nflux_err           41029\ntimecorr               0\ncadenceno              0\nsap_flux           21905\nsap_flux_err       21905\npdcsap_flux        41029\npdcsap_flux_err    41029\nquality                0\ntime                   0\nsource_file            0\ndtype: int64\n------\n\nnum of dups in df   =   0\n------\n\nUnique values :  \n flux               1489542\nflux_err           1549840\ntimecorr           1558093\ncadenceno            20257\nsap_flux           1628077\nsap_flux_err       1595392\npdcsap_flux        1489542\npdcsap_flux_err    1549840\nquality                  7\ntime               2025554\nsource_file            100\ndtype: int64\n------\n\n null values =  True\n------\n\"  ?   \" values present =  \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\ncentroid_col       0\ncentroid_row       0\nsap_flux           0\nsap_flux_err       0\nsap_bkg            0\nsap_bkg_err        0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\npsf_centr1         0\npsf_centr1_err     0\npsf_centr2         0\npsf_centr2_err     0\nmom_centr1         0\nmom_centr1_err     0\nmom_centr2         0\nmom_centr2_err     0\npos_corr1          0\npos_corr2          0\ntime               0\nsource_file        0\ndtype: int64\n------\n\"   unknown   \" values present =  \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\nsap_flux           0\nsap_flux_err       0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\ntime               0\nsource_file        0\ndtype: int64\n------\n    NA     values present = \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\nsap_flux           0\nsap_flux_err       0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\ntime               0\nsource_file        0\ndtype: int64\n------\n    none    values present = \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\nsap_flux           0\nsap_flux_err       0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\ntime               0\nsource_file        0\ndtype: int64\n\n\n2.5 Further Data Insights\nWe have over 2 million rows of data and 11 columns of interest.\n\n‘pdcsapflux’\n\nThis will be the main column of interest for us, i.e. the cleaned light flux data.\nMissing values in this column of interest are ~ 41K. Its a big enough number so need to consider implication of deleting. If we do delete them we do still have a very large dataset though.\nTo replace missing flux data, interpolating perhaps would provide not ‘true’ data in that column. There is a trade off having real data vs artificial data. This is a critical column and we can really only have valid data present in it.\nAfter consideration we will drop empty values in ‘dscsap_flux’ column, this is to avoid erroneous data and we can still have a significant amount of data available to us for analysis.\nTo replace missing flux data would provide not true data in that column, this is a critical column and we can only have valid data present in it.\nDon’t want to over clean the data as not 100% confident in what is in it and don’t want to remove anything that is potentially meaningful, i.e. an outlier could be a value that we are looking to identify…\n\n‘sap_flux’ and ‘flux’ These also contain missing values. We will remove the ‘pdc’ values first and see if that affects these, they may be related.\n\nThe associated error values ’_err’ also havign missinfg values, again we’ll see how removing the pdc values affects these, before deciding how to deal with them. We do not intend using them in analysis at this stage so not a priority.\n2.6 Plot for how the data looks for different star types\n\n# Plot for how the data looks for different star types just to get a feel for it\nfor star in clean_df[\"source_file\"].unique()[:3]:  # e.g., first 3 stars\n    star_df = clean_df[clean_df[\"source_file\"] == star]\n    plt.figure(figsize=(10, 4))\n    plt.plot(star_df[\"time\"], star_df[\"pdcsap_flux\"], '.', markersize=2)\n    plt.title(f\"Light Curve for {star}\")\n    plt.xlabel(\"Time (BTJD)\")\n    plt.ylabel(\"pdcsap_flux\")\n    plt.grid(True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.7 Plots of distribution of data per column\n\n# look at distribution of data\nnumeric_cols = clean_df.select_dtypes(include='number').columns\n\nfor col in numeric_cols:\n    plt.figure(figsize=(10, 3))\n    clean_df[col].hist(bins=100)\n    plt.title(f\"Distribution of {col}\")\n    plt.xlabel(col)\n    plt.ylabel(\"Count\")\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.8 Plots of outliers in the column data\n\nnumeric_columns = clean_df.select_dtypes(include=['number']).columns\nsample_df = clean_df.sample(frac=0.01, random_state=42)\n\nfor col in numeric_columns:\n    plt.figure(figsize=(10, 2))\n    plt.boxplot(sample_df[col].dropna(), vert=False)\n    plt.title(f'Boxplot of {col}')\n    plt.xlabel(col)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.9 Further Data Insights from plots\nTime - gaps could be down time in instruments where no data taken\nOutliers ??? have outliers how to deal with them… again dont’ want to mess with the actual real data too much as not familaiur enough with it . want to keep as much raw real data as possible. could MinMaxScaler or winzorise…. what is best"
  },
  {
    "objectID": "notebooks/exoplanet_id_clustering_final.html#step-3---clean-data",
    "href": "notebooks/exoplanet_id_clustering_final.html#step-3---clean-data",
    "title": "TU257 - Assignment B",
    "section": "Step 3 - Clean data",
    "text": "Step 3 - Clean data\n3.1 Drop empty columns from ‘pdcsap_flux’ and verify successful\n\n# craete a new df off rclean_df so we cna have a clean df to work in and roll back to thsi point if we needed\nclean_col_rem_df=clean_df.copy()\n\n\n#drop empty coluimns in pdcsap_flux\nclean_col_rem_df = clean_col_rem_df.dropna(subset=[\"pdcsap_flux\"])\n\n# # fill missing value error columns with median values\n# # we will use median to avoid any potential skewness or outlier values\n# clean2_df[\"pdcsap_flux_err\"].fillna(clean2_df[\"pdcsap_flux_err\"].median(), inplace=True)\n\n\nprint (\"Rows     : \" , clean_col_rem_df.shape[0])\nprint (\"Columns  : \" , clean_col_rem_df.shape[1])\nprint('------')\nprint (\"\\nFeatures : \\n\" , clean_col_rem_df.columns.tolist())\nprint('------')\nprint (\"\\nMissing values :  \", clean_col_rem_df.isnull().sum().values.sum())\nprint('------')\nprint (\"\\nMissing values per column:  \\n\", clean_col_rem_df.isnull().sum())\nprint('------')\nprint('\\nnum of dups in df   =   {}'.format(clean_col_rem_df.duplicated().sum()))\nprint('------')\nprint (\"\\nUnique values :  \\n\", clean_col_rem_df.nunique())\nprint('------')\nprint('\\n null values = ', clean_col_rem_df.isnull().values.any())\nprint('------')\nclean_col_rem_df_missing = (df=='?').sum()\nprint('\\\"  ?   \\\" values present =  \\n',clean_col_rem_df_missing)\nprint('------')\nclean_col_rem_df_unknown = (clean_col_rem_df=='unknown').sum()\nprint('\\\"   unknown   \\\" values present =  \\n',clean_col_rem_df_unknown)\nprint('------')\nclean_col_rem_df_na = clean_col_rem_df.isin(['N/A', 'N\\\\A', 'NA', 'n/a', 'n\\\\A', 'na', 'N_A']).sum()\nprint('    NA     values present = \\n', clean_col_rem_df_na)\nprint('------')\nclean_col_rem_df_none = clean_col_rem_df.isin(['None', 'none']).sum()\nprint('    none    values present = \\n', clean_col_rem_df_none)\n\nRows     :  1984671\nColumns  :  11\n------\n\nFeatures : \n ['flux', 'flux_err', 'timecorr', 'cadenceno', 'sap_flux', 'sap_flux_err', 'pdcsap_flux', 'pdcsap_flux_err', 'quality', 'time', 'source_file']\n------\n\nMissing values :   0\n------\n\nMissing values per column:  \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\nsap_flux           0\nsap_flux_err       0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\ntime               0\nsource_file        0\ndtype: int64\n------\n\nnum of dups in df   =   0\n------\n\nUnique values :  \n flux               1489542\nflux_err           1549840\ntimecorr           1529090\ncadenceno            20257\nsap_flux           1612731\nsap_flux_err       1577096\npdcsap_flux        1489542\npdcsap_flux_err    1549840\nquality                  2\ntime               1984525\nsource_file            100\ndtype: int64\n------\n\n null values =  False\n------\n\"  ?   \" values present =  \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\ncentroid_col       0\ncentroid_row       0\nsap_flux           0\nsap_flux_err       0\nsap_bkg            0\nsap_bkg_err        0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\npsf_centr1         0\npsf_centr1_err     0\npsf_centr2         0\npsf_centr2_err     0\nmom_centr1         0\nmom_centr1_err     0\nmom_centr2         0\nmom_centr2_err     0\npos_corr1          0\npos_corr2          0\ntime               0\nsource_file        0\ndtype: int64\n------\n\"   unknown   \" values present =  \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\nsap_flux           0\nsap_flux_err       0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\ntime               0\nsource_file        0\ndtype: int64\n------\n    NA     values present = \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\nsap_flux           0\nsap_flux_err       0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\ntime               0\nsource_file        0\ndtype: int64\n------\n    none    values present = \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\nsap_flux           0\nsap_flux_err       0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\ntime               0\nsource_file        0\ndtype: int64\n\n\n3.2 Further Data Insight\nRemoving the pdcsap_flux missing rows has removed the missing sap and flux and _err data also. Don’t have any missing values on those rows now, which is good.\nNormalisation is important as different stars have different brightness and is a range of values, bright star will skew dat fomr stars not as bright, so need tonormalise/stanmdarise this flux data so all balanced around a central norm i.e. baseline of 1.0\nwinsorisation - address outliers from influeicng analysis otucomes we’ll apply to numerical columns we’ll exclude the following form winsoristation - ‘quality’ - flags about quality 0, 1. need to keep as is - ‘cadenceno’ - more or less an indes - ‘time’ - a set time value, a real number don’t want to change - ‘timecorr’ - as above - ‘source_file’ - non-numerical, relates to star ID\n3.3 Normalise the ‘pdcsap_sap_flux’ data and verify the chnages in a plot\n\n#Normalise Data\n\n# normalises uses the median per star and we groupby star using the source_file column\n# divide each flux value by that star's median flux value\n# we use transform to make sure the result keeps the original df shape and alignment\n# and we create a new column, 'pdcsap_flux_norm', to store the normalised data, sowe dont' lose the original\n# will create new df to normalise and retain cleaned df in case need\nclean_norm_df = clean_col_rem_df.copy()\nclean_norm_df['pdcsap_flux_norm'] = clean_col_rem_df.groupby(\"source_file\")['pdcsap_flux'].transform(lambda x: x / x.median())\n\n\n# Choose a single sample star which we'll pot before and after tocheck\n# cnba use source file toget individual star data\nexample_star = clean_norm_df[\"source_file\"].unique()[0]\n\n# Filter data for that star\ndf_star = clean_norm_df[clean_norm_df[\"source_file\"] == example_star]\n\n# Plot original vs normalized pdcsap_flux\nplt.figure(figsize=(14, 5))\n\n# Original flux\nplt.subplot(1, 2, 1)\nplt.plot(df_star[\"time\"], df_star[\"pdcsap_flux\"], color='blue')\nplt.title(\"Original pdcsap_flux\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Flux\")\n\n# Normalized flux\nplt.subplot(1, 2, 2)\nplt.plot(df_star[\"time\"], df_star[\"pdcsap_flux_norm\"], color='green')\nplt.title(\"Normalized pdcsap_flux (per star median)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Normalized Flux\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNormalisation plots above:\n\nbefore - large range of data on y axis, values spread are much larger\nAfter - data centred around a baseline of 1.0. Spread of data is much smalle around the baseline\n\n3.4 Winsorise selected numerical columns and verify the chnages in a plot\n\n# Winsorise Data\n\n# 7. Winzorising the data for extreme values\n# from scipy.stats.mstats import winsorize\n\n# crteate new df to work in\nclean_winsor_df=clean_norm_df.copy()\n\n# identify cols to include\nnum_cols_1 = clean_norm_df.select_dtypes(include=['number']).columns\ncols_to_excl=['quality', 'cadenceno', 'time', 'timecorr', 'source_file']\nnum_cols_2 = [col for col in num_cols_1 if col not in cols_to_excl]\nprint(num_cols_1)\nprint(num_cols_2)\n\n# Define 1% limit on both tails)\nlower = 0.01  # 1% percentile lower tail\nupper = 0.99  # 99% percentile upper tail\n\n# Apply winzoristiation \nfor col in num_cols_2:\n    col_data=clean_winsor_df[col].values\n    winsorised=winsorize(col_data, limits=(lower, 1-upper))\n    clean_winsor_df[col] = winsorised.filled()\n\nIndex(['flux', 'flux_err', 'timecorr', 'cadenceno', 'sap_flux', 'sap_flux_err',\n       'pdcsap_flux', 'pdcsap_flux_err', 'quality', 'time',\n       'pdcsap_flux_norm'],\n      dtype='object')\n['flux', 'flux_err', 'sap_flux', 'sap_flux_err', 'pdcsap_flux', 'pdcsap_flux_err', 'pdcsap_flux_norm']\n\n\n\nclean_winsor_df.describe().transpose()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nflux\n1984671.0\n5.944763e+05\n1.792601e+06\n2.808894e+01\n5.615400e+03\n6.363539e+04\n9.444168e+04\n1.025003e+07\n\n\nflux_err\n1984671.0\n4.904461e+01\n6.773579e+01\n5.003173e+00\n1.114961e+01\n2.935878e+01\n3.557361e+01\n3.462266e+02\n\n\ntimecorr\n1984671.0\n4.448540e-03\n6.400926e-04\n3.200641e-03\n3.988675e-03\n4.219107e-03\n4.973155e-03\n5.704571e-03\n\n\ncadenceno\n1984671.0\n1.803457e+06\n5.911514e+03\n1.793292e+06\n1.798263e+06\n1.803382e+06\n1.808503e+06\n1.814032e+06\n\n\nsap_flux\n1984671.0\n5.852072e+05\n1.770710e+06\n4.785327e+01\n5.126049e+03\n5.997101e+04\n8.947065e+04\n1.018834e+07\n\n\nsap_flux_err\n1984671.0\n4.686832e+01\n6.746164e+01\n2.796352e+00\n8.886965e+00\n2.758974e+01\n3.350297e+01\n3.422422e+02\n\n\npdcsap_flux\n1984671.0\n5.944763e+05\n1.792601e+06\n2.808894e+01\n5.615400e+03\n6.363539e+04\n9.444168e+04\n1.025003e+07\n\n\npdcsap_flux_err\n1984671.0\n4.904461e+01\n6.773579e+01\n5.003173e+00\n1.114961e+01\n2.935878e+01\n3.557361e+01\n3.462266e+02\n\n\nquality\n1984671.0\n3.201498e-01\n1.279900e+01\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n5.120000e+02\n\n\ntime\n1984671.0\n3.732261e+03\n8.210352e+00\n3.718142e+03\n3.725047e+03\n3.732157e+03\n3.739269e+03\n3.746948e+03\n\n\npdcsap_flux_norm\n1984671.0\n9.995998e-01\n3.813760e-02\n8.147628e-01\n9.992826e-01\n1.000000e+00\n1.000718e+00\n1.177999e+00\n\n\n\n\n\n\n\n\n# quick check to see if data is as we expect post winsorising\n# we'll plot pdc flus column to see difference as its our main column of interest\ncolumn_to_plot = 'pdcsap_flux_norm'  # or any other from num_cols_2\n\n# Create histograms before and after winsorizstion\nplt.figure(figsize=(12, 5))\n\n# Before winsorization\nplt.subplot(1, 2, 1)\nplt.hist(clean_norm_df[column_to_plot], bins=100, color='skyblue', edgecolor='black')\nplt.title(f'Before Winsorisation: {column_to_plot}')\nplt.xlabel(column_to_plot)\nplt.ylabel('Frequency')\n\n# After winsorization\nplt.subplot(1, 2, 2)\nplt.hist(clean_winsor_df[column_to_plot], bins=100, color='lightgreen', edgecolor='black')\nplt.title(f'After Winsorisation: {column_to_plot}')\nplt.xlabel(column_to_plot)\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWinsorisation plots above:\n\nbefore - goes up to 1.75 million form zero\nAfter - outliers outside of 1% percentile and 99% percentile have been removed. Data is tighter around 1.0"
  },
  {
    "objectID": "notebooks/exoplanet_id_clustering_final.html#step-4---extract-a-feature-from-the-data-to-cluster",
    "href": "notebooks/exoplanet_id_clustering_final.html#step-4---extract-a-feature-from-the-data-to-cluster",
    "title": "TU257 - Assignment B",
    "section": "Step 4 - Extract a feature from the Data to cluster",
    "text": "Step 4 - Extract a feature from the Data to cluster\nAs it is time series data and we have multiple values per star we need to combines those values to a single, sumarised value per star so we can use them in clustering. we will extract those now in this section.\nwe’ll lok at number of dips in pdcsap_flux and time\n4.1 Create new df of latest to work in\n\n# create new df to work on from thsi point and retain latest one in case we need to return to a checkpoint in time\nextract_feature_df=clean_winsor_df.copy()\n\n4.2 Determine light curve dip data values from ‘pdcsap_flux_norm’ and Plot them to verify data is as expect\n\n# play around wiht prominence value \n# &gt;&gt; 0.005 =  no dips, \n# &gt;&gt; 0.004 = 2, 1, and 6 dips\n# &gt;&gt; 0.003 = 21, 42 and 25 dips\n# Trial and error determien what a good value is and we're not including noise but real dip values and drops in intensity of light\n# decided to go with 0.004 value to reduce the amount of false positives\ndef plot_light_curve_with_dips(df, star_id, flux_col='pdcsap_flux_norm', prominence=0.004):\n    star_data = df[df['source_file'] == star_id]\n    flux = star_data[flux_col].values\n    time = star_data['time'].values\n\n    # Find dips\n    dips, _ = find_peaks(-flux, prominence=prominence)\n    dip_count = len(dips)\n\n    # Plot\n    plt.figure(figsize=(10, 4))\n    plt.plot(time, flux, label='Flux')\n    plt.plot(time[dips], flux[dips], 'ro', label='Detected Dips')\n    plt.xlabel('Time (BTJD)')\n    plt.ylabel('Normalized Flux')\n    plt.title(f\"Light Curve: {star_id}  |  Dips Found: {dip_count}\")\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n# [5] [SciPy - find_peaks](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html#scipy.signal.find_peaks)\n\n\n# Get 3 unique stars to plot\nsample_stars = extract_feature_df['source_file'].unique()[:3]\n\n# Plot them\nfor star in sample_stars:\n    plot_light_curve_with_dips(extract_feature_df, star)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nextract_feature_df.shape\n\n(1984671, 12)\n\n\n4.3 Calculate and Add flux dip data, ‘num_flux_dips’ to new column in dataframe\n\n# Create a dictionary to store dip counts\nnum_flux_dips = {}\n\n# Group by source_file (one light curve per star)\nfor source, group in extract_feature_df.groupby(\"source_file\"):\n    flux = group[\"pdcsap_flux_norm\"].values\n    # we invert the flux as per documentation toidentify the valleys/dips\n    # set value at 0.004 as per earlier verification checks\n    dips, _ = find_peaks(-flux, prominence=0.004) \n    num_flux_dips[source] = len(dips)\n\n\n# Convert to DataFrame which we cna ten merge\nnum_flux_dips_df = pd.DataFrame(list(num_flux_dips.items()), columns=[\"source_file\", \"num_flux_dips\"])\n\n\n# Merge with your main dataframe on source_file)\nextract_feature_df = extract_feature_df.merge(num_flux_dips_df, on=\"source_file\", how=\"left\")\n\n\nprint (\"Rows     : \" , extract_feature_df.shape[0])\nprint (\"Columns  : \" , extract_feature_df.shape[1])\nprint('------')\nprint (\"\\nFeatures : \\n\" , extract_feature_df.columns.tolist())\nprint('------')\nprint (\"\\nMissing values :  \", extract_feature_df.isnull().sum().values.sum())\nprint('------')\nprint (\"\\nMissing values per column:  \\n\", extract_feature_df.isnull().sum())\nprint('------')\nprint('\\nnum of dups in df   =   {}'.format(extract_feature_df.duplicated().sum()))\nprint('------')\nprint (\"\\nUnique values :  \\n\", extract_feature_df.nunique())\nprint('------')\nprint('\\n null values = ', extract_feature_df.isnull().values.any())\nprint('------')\nextract_feature_df_missing = (df=='?').sum()\nprint('\\\"  ?   \\\" values present =  \\n',extract_feature_df_missing)\nprint('------')\nextract_feature_df_unknown = (extract_feature_df=='unknown').sum()\nprint('\\\"   unknown   \\\" values present =  \\n',extract_feature_df_unknown)\nprint('------')\nextract_feature_df_na = extract_feature_df.isin(['N/A', 'N\\\\A', 'NA', 'n/a', 'n\\\\A', 'na', 'N_A']).sum()\nprint('    NA     values present = \\n', extract_feature_df_na)\nprint('------')\nextract_feature_df_none = extract_feature_df.isin(['None', 'none']).sum()\nprint('    none    values present = \\n', extract_feature_df_none)\n\nRows     :  1984671\nColumns  :  13\n------\n\nFeatures : \n ['flux', 'flux_err', 'timecorr', 'cadenceno', 'sap_flux', 'sap_flux_err', 'pdcsap_flux', 'pdcsap_flux_err', 'quality', 'time', 'source_file', 'pdcsap_flux_norm', 'num_flux_dips']\n------\n\nMissing values :   0\n------\n\nMissing values per column:  \n flux                0\nflux_err            0\ntimecorr            0\ncadenceno           0\nsap_flux            0\nsap_flux_err        0\npdcsap_flux         0\npdcsap_flux_err     0\nquality             0\ntime                0\nsource_file         0\npdcsap_flux_norm    0\nnum_flux_dips       0\ndtype: int64\n------\n\nnum of dups in df   =   0\n------\n\nUnique values :  \n flux                1451680\nflux_err            1514753\ntimecorr            1529090\ncadenceno             20257\nsap_flux            1575093\nsap_flux_err        1541825\npdcsap_flux         1451680\npdcsap_flux_err     1514753\nquality                   2\ntime                1984525\nsource_file             100\npdcsap_flux_norm     399773\nnum_flux_dips            61\ndtype: int64\n------\n\n null values =  False\n------\n\"  ?   \" values present =  \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\ncentroid_col       0\ncentroid_row       0\nsap_flux           0\nsap_flux_err       0\nsap_bkg            0\nsap_bkg_err        0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\npsf_centr1         0\npsf_centr1_err     0\npsf_centr2         0\npsf_centr2_err     0\nmom_centr1         0\nmom_centr1_err     0\nmom_centr2         0\nmom_centr2_err     0\npos_corr1          0\npos_corr2          0\ntime               0\nsource_file        0\ndtype: int64\n------\n\"   unknown   \" values present =  \n flux                0\nflux_err            0\ntimecorr            0\ncadenceno           0\nsap_flux            0\nsap_flux_err        0\npdcsap_flux         0\npdcsap_flux_err     0\nquality             0\ntime                0\nsource_file         0\npdcsap_flux_norm    0\nnum_flux_dips       0\ndtype: int64\n------\n    NA     values present = \n flux                0\nflux_err            0\ntimecorr            0\ncadenceno           0\nsap_flux            0\nsap_flux_err        0\npdcsap_flux         0\npdcsap_flux_err     0\nquality             0\ntime                0\nsource_file         0\npdcsap_flux_norm    0\nnum_flux_dips       0\ndtype: int64\n------\n    none    values present = \n flux                0\nflux_err            0\ntimecorr            0\ncadenceno           0\nsap_flux            0\nsap_flux_err        0\npdcsap_flux         0\npdcsap_flux_err     0\nquality             0\ntime                0\nsource_file         0\npdcsap_flux_norm    0\nnum_flux_dips       0\ndtype: int64\n\n\n4.4 Calculate the variability of the flux per star, we’ll use the standard deviation to represent this, ‘flux_std’\n\nflux_std_df = extract_feature_df.groupby(\"source_file\").agg(flux_std=('pdcsap_flux_norm', 'std')).reset_index()\n\n4.5 Calculate average dip duration per star, ‘dip_duration’\n\n# we cna calculate the width of the invert peaks, which are the dips\ndip_durations = []\n\nfor source, group in extract_feature_df.groupby(\"source_file\"):\n    flux = group[\"pdcsap_flux_norm\"].values\n    time = group[\"time\"].values\n    dips, _ = find_peaks(-flux, prominence=0.004)\n    \n    # Calculate widths in time units\n    results_half = peak_widths(-flux, dips, rel_height=0.5)\n    widths = results_half[0]  # widths in data points\n\n    # convert to time units using time difference\n    if len(widths) &gt; 0:\n        avg_duration = np.mean(widths * np.median(np.diff(time)))\n    else:\n        avg_duration = 0\n    \n    dip_durations.append({\"source_file\": source, \"avg_dip_duration\": avg_duration})\n\ndip_duration_df = pd.DataFrame(dip_durations)\n\n4.6 Merge all calculated per star data into a dataframe and verify\n\n# take source file and num_flux_dips from df and put them into a new df to use in clustering\nx_features_df = extract_feature_df[['source_file', 'num_flux_dips']].drop_duplicates()\n\n# add flus_std column data\nx_features_df = x_features_df.merge(flux_std_df, on=\"source_file\", how=\"left\")\n\n# add dip duration data \nx_features_df = x_features_df.merge(dip_duration_df, on=\"source_file\", how=\"left\")\n\n\nx_features_df.head()\n\n\n\n\n\n\n\n\nsource_file\nnum_flux_dips\nflux_std\navg_dip_duration\n\n\n\n\n0\ntess2025042113628-s0089-0000000000872755-0286-...\n2\n0.000530\n0.090759\n\n\n1\ntess2025042113628-s0089-0000000000991159-0286-...\n1\n0.000536\n0.011295\n\n\n2\ntess2025042113628-s0089-0000000000990995-0286-...\n6\n0.000559\n0.013028\n\n\n3\ntess2025042113628-s0089-0000000001228450-0286-...\n0\n0.000324\n0.000000\n\n\n4\ntess2025042113628-s0089-0000000001137183-0286-...\n2429\n0.002072\n0.003628\n\n\n\n\n\n\n\n\nprint (\"Rows     : \" , x_features_df.shape[0])\nprint (\"Columns  : \" , x_features_df.shape[1])\nprint('------')\nprint (\"\\nFeatures : \\n\" , x_features_df.columns.tolist())\nprint('------')\nprint (\"\\nMissing values :  \", x_features_df.isnull().sum().values.sum())\nprint('------')\nprint (\"\\nMissing values per column:  \\n\", x_features_df.isnull().sum())\nprint('------')\nprint('\\nnum of dups in df   =   {}'.format(x_features_df.duplicated().sum()))\nprint('------')\nprint (\"\\nUnique values :  \\n\", x_features_df.nunique())\nprint('------')\nprint('\\n null values = ', x_features_df.isnull().values.any())\nprint('------')\nx_features_df_missing = (df=='?').sum()\nprint('\\\"  ?   \\\" values present =  \\n',x_features_df_missing)\nprint('------')\nx_features_df_unknown = (x_features_df=='unknown').sum()\nprint('\\\"   unknown   \\\" values present =  \\n',x_features_df_unknown)\nprint('------')\nx_features_df_na = x_features_df.isin(['N/A', 'N\\\\A', 'NA', 'n/a', 'n\\\\A', 'na', 'N_A']).sum()\nprint('    NA     values present = \\n', x_features_df_na)\nprint('------')\nx_features_df_none = x_features_df.isin(['None', 'none']).sum()\nprint('    none    values present = \\n', x_features_df_none)\n\nRows     :  100\nColumns  :  4\n------\n\nFeatures : \n ['source_file', 'num_flux_dips', 'flux_std', 'avg_dip_duration']\n------\n\nMissing values :   0\n------\n\nMissing values per column:  \n source_file         0\nnum_flux_dips       0\nflux_std            0\navg_dip_duration    0\ndtype: int64\n------\n\nnum of dups in df   =   0\n------\n\nUnique values :  \n source_file         100\nnum_flux_dips        61\nflux_std            100\navg_dip_duration     86\ndtype: int64\n------\n\n null values =  False\n------\n\"  ?   \" values present =  \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\ncentroid_col       0\ncentroid_row       0\nsap_flux           0\nsap_flux_err       0\nsap_bkg            0\nsap_bkg_err        0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\npsf_centr1         0\npsf_centr1_err     0\npsf_centr2         0\npsf_centr2_err     0\nmom_centr1         0\nmom_centr1_err     0\nmom_centr2         0\nmom_centr2_err     0\npos_corr1          0\npos_corr2          0\ntime               0\nsource_file        0\ndtype: int64\n------\n\"   unknown   \" values present =  \n source_file         0\nnum_flux_dips       0\nflux_std            0\navg_dip_duration    0\ndtype: int64\n------\n    NA     values present = \n source_file         0\nnum_flux_dips       0\nflux_std            0\navg_dip_duration    0\ndtype: int64\n------\n    none    values present = \n source_file         0\nnum_flux_dips       0\nflux_std            0\navg_dip_duration    0\ndtype: int64\n\n\n4.7 Data Insights\n\nwe have 100 unique source file values which means data 100 stars\nwe have 61 flux dip values, which hopefully is a reasonable spread\nwe have 86 dip duration values\nwe have 100 flux_std values\nThere is a range in the date so we will need to normalise the data\nthere are 0.0 values present, they are zero values and not missing values, so we will leave them as trying to keep as much original data as possible\n\n4.8 Normalise the dataframe and verify it is as we expect\n\nfeatures = ['num_flux_dips', 'flux_std', 'avg_dip_duration']\nscaler = MinMaxScaler()\nx_features_norm_df = scaler.fit_transform(x_features_df[features])\n\n\n# convetr back to datafram as scaler.fit_transform gives us a numpy array\nx_features_norm_df = pd.DataFrame(x_features_norm_df, columns=features, index=x_features_df.index)\n\n# add back in the source _file column as lost it in the normalization\nx_features_norm_df = pd.DataFrame(x_features_norm_df, columns=features, index=x_features_df.index)\nx_features_norm_df[\"source_file\"] = x_features_df[\"source_file\"].values\n\n# quick look at its shape\nx_features_norm_df.shape\n\n(100, 4)\n\n\n\nprint (\"Rows     : \" , x_features_norm_df.shape[0])\nprint (\"Columns  : \" , x_features_norm_df.shape[1])\nprint('------')\nprint (\"\\nFeatures : \\n\" , x_features_norm_df.columns.tolist())\nprint('------')\nprint (\"\\nMissing values :  \", x_features_norm_df.isnull().sum().values.sum())\nprint('------')\nprint (\"\\nMissing values per column:  \\n\", x_features_norm_df.isnull().sum())\nprint('------')\nprint('\\nnum of dups in df   =   {}'.format(x_features_norm_df.duplicated().sum()))\nprint('------')\nprint (\"\\nUnique values :  \\n\", x_features_norm_df.nunique())\nprint('------')\nprint('\\n null values = ', x_features_norm_df.isnull().values.any())\nprint('------')\nx_features_norm_df_missing = (df=='?').sum()\nprint('\\\"  ?   \\\" values present =  \\n',x_features_df_missing)\nprint('------')\nx_features_norm_df_unknown = (x_features_norm_df=='unknown').sum()\nprint('\\\"   unknown   \\\" values present =  \\n',x_features_norm_df_unknown)\nprint('------')\nx_features_norm_df_na = x_features_norm_df.isin(['N/A', 'N\\\\A', 'NA', 'n/a', 'n\\\\A', 'na', 'N_A']).sum()\nprint('    NA     values present = \\n', x_features_norm_df_na)\nprint('------')\nx_features_norm_df_none = x_features_norm_df.isin(['None', 'none']).sum()\nprint('    none    values present = \\n', x_features_norm_df_none)\n\nRows     :  100\nColumns  :  4\n------\n\nFeatures : \n ['num_flux_dips', 'flux_std', 'avg_dip_duration', 'source_file']\n------\n\nMissing values :   0\n------\n\nMissing values per column:  \n num_flux_dips       0\nflux_std            0\navg_dip_duration    0\nsource_file         0\ndtype: int64\n------\n\nnum of dups in df   =   0\n------\n\nUnique values :  \n num_flux_dips        61\nflux_std            100\navg_dip_duration     86\nsource_file         100\ndtype: int64\n------\n\n null values =  False\n------\n\"  ?   \" values present =  \n flux               0\nflux_err           0\ntimecorr           0\ncadenceno          0\ncentroid_col       0\ncentroid_row       0\nsap_flux           0\nsap_flux_err       0\nsap_bkg            0\nsap_bkg_err        0\npdcsap_flux        0\npdcsap_flux_err    0\nquality            0\npsf_centr1         0\npsf_centr1_err     0\npsf_centr2         0\npsf_centr2_err     0\nmom_centr1         0\nmom_centr1_err     0\nmom_centr2         0\nmom_centr2_err     0\npos_corr1          0\npos_corr2          0\ntime               0\nsource_file        0\ndtype: int64\n------\n\"   unknown   \" values present =  \n num_flux_dips       0\nflux_std            0\navg_dip_duration    0\nsource_file         0\ndtype: int64\n------\n    NA     values present = \n num_flux_dips       0\nflux_std            0\navg_dip_duration    0\nsource_file         0\ndtype: int64\n------\n    none    values present = \n num_flux_dips       0\nflux_std            0\navg_dip_duration    0\nsource_file         0\ndtype: int64"
  },
  {
    "objectID": "notebooks/exoplanet_id_clustering_final.html#step-5---clustering",
    "href": "notebooks/exoplanet_id_clustering_final.html#step-5---clustering",
    "title": "TU257 - Assignment B",
    "section": "Step 5 - Clustering",
    "text": "Step 5 - Clustering\n5.1 - Create a new dataframe with one row per star on our feature\n\n# create a new dataframe to work in\nx_df = x_features_norm_df.copy()\n\n\n# drop sopurce_file as kmeans fit expects al numerical data\nx_df = x_df.drop(columns=[\"source_file\"])\n\n\nx_df.head()\n\n\n\n\n\n\n\n\nnum_flux_dips\nflux_std\navg_dip_duration\n\n\n\n\n0\n0.000288\n0.002428\n0.022857\n\n\n1\n0.000144\n0.002475\n0.002845\n\n\n2\n0.000865\n0.002629\n0.003281\n\n\n3\n0.000000\n0.001005\n0.000000\n\n\n4\n0.350353\n0.013124\n0.000914\n\n\n\n\n\n\n\n\n# scaler = MinMaxScaler()\n\n# # Normalize 'num_flux_dips'\n# scaler.fit(x_df[['num_flux_dips']])\n# x_df['num_flux_dips'] = scaler.transform(x_df[['num_flux_dips']])\n\n# # Normalize 'avg_dip_duration'\n# scaler.fit(x_df[['avg_dip_duration']])\n# x_df['avg_dip_duration'] = scaler.transform(x_df[['avg_dip_duration']])\n\n# # Normalize 'flux_std'\n# scaler.fit(x_df[['flux_std']])\n# x_df['flux_std'] = scaler.transform(x_df[['flux_std']])\n\n\n# creat6e new dataframe that will contain one row per star\nx_df = x_df[[\"source_file\", \"num_flux_dips\"]].drop_duplicates(subset=[\"source_file\"])\n\n\n#  we just need the numerical column(s)\nx_features = x_df[[\"num_flux_dips\"]]\n\n\nx_df.shape\n\n(100, 3)\n\n\n5.2 Plot elbow curve to determine appropriate cluster count\n\n# Try different cluster counts\nsse = []\nk_range = range(1, 10)\n\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=42)\n    km.fit(x_df)\n    sse.append(km.inertia_)  # Sum of squared distances\n\n# Plot the elbow curve\nplt.figure(figsize=(8, 4))\nplt.plot(k_range, sse, marker='o')\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Sum of Squared Errors (SSE)\")\nplt.title(\"Elbow Method for Optimal k\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n5.3 Create KMeans cluster and analyze data\n\n# # initialise the kmeans model with 2 clusters and we're just using num_flux_dips column to cerry out clustering\n# # we'll add thre resulting cluster label of 1 or 0 as a new column in x_features called cluster\n# kmeans = KMeans(n_clusters=3, random_state=42)\n# x_df[\"cluster\"] = kmeans.fit_predict(x_df[[\"num_flux_dips\", \"avg_dip_duration\", \"flux_std\"]])\n\n#x_df([\"cluster\"].value_counts())\n\n\n#Now create a K-means model containing 2 clustsers\n# we dicded ont his after tryign all values and also after silhouette plot calculation\nkm = KMeans(n_clusters=2)\ny_predicted = km.fit_predict(x_df[['num_flux_dips',\"avg_dip_duration\", \"flux_std\"]])\n#display the predicted clusters\ny_predicted\n\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n#Add the clusters to the original DF, to have all the data together\n#x_df['cluster']=y_predicted\nx_df.head()\n\n\n\n\n\n\n\n\nnum_flux_dips\nflux_std\navg_dip_duration\ncluster\n\n\n\n\n0\n0.000288\n0.002428\n0.022857\n0\n\n\n1\n0.000144\n0.002475\n0.002845\n0\n\n\n2\n0.000865\n0.002629\n0.003281\n0\n\n\n3\n0.000000\n0.001005\n0.000000\n0\n\n\n4\n0.350353\n0.013124\n0.000914\n0\n\n\n\n\n\n\n\n\n#We can examine different aspects of the model\n#Here we list the Centroids for the 3 clusters\nkm.cluster_centers_\n\narray([[7.72944401e-02, 6.03345047e-02, 1.50447120e-02],\n       [9.23169383e-01, 6.89811090e-04, 6.13679104e-01]])\n\n\n5.4 Plot Kmeans cluster data\n\n# Separate out each cluster for custom formatting\ndf0 = x_df[x_df.cluster == 0]\ndf1 = x_df[x_df.cluster == 1]\n# df2 = x_df[x_df.cluster == 2]\n\n# Plot each cluster with a different color\nplt.scatter(df0['avg_dip_duration'], df0['num_flux_dips'], color='green', label='Cluster 0')\nplt.scatter(df1['avg_dip_duration'], df1['num_flux_dips'], color='red', label='Cluster 1')\n# plt.scatter(df2['avg_dip_duration'], df2['num_flux_dips'], color='blue', label='Cluster 2')\n\n# Plot centroids\nplt.scatter(km.cluster_centers_[:,1], km.cluster_centers_[:,0], color='purple', marker='+', s=200, label='Centroids')\n\n# Label axes\nplt.xlabel('Average Dip Duration')\nplt.ylabel('Number of Flux Dips')\nplt.title('K-Means Clustering of TESS Stars')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=x_df, x=\"avg_dip_duration\", y=\"num_flux_dips\", hue=\"cluster\", palette=\"Set1\")\n\n# Plot centroids\nplt.scatter(km.cluster_centers_[:,1], km.cluster_centers_[:,0], color='purple', marker='+', s=200, label='Centroids')\n\nplt.title(\"Clusters by Number of Flux Dips\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=x_df, x=\"flux_std\", y=\"num_flux_dips\", hue=\"cluster\", palette=\"Set1\")\n# Plot centroids\nplt.scatter(km.cluster_centers_[:,1], km.cluster_centers_[:,0], color='purple', marker='+', s=200, label='Centroids')\n\nplt.title(\"Clusters by Number of Flux Dips\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=x_df, x=\"flux_std\", y=\"avg_dip_duration\", hue=\"cluster\", palette=\"Set1\")\n\n# Plot centroids\nplt.scatter(km.cluster_centers_[:,1], km.cluster_centers_[:,0], color='purple', marker='+', s=200, label='Centroids')\n\nplt.title(\"Clusters by Number of Flux Dips\")\nplt.show()\n\n\n\n\n\n\n\n\n5.5 Plot Kmeans cluster data in 3D\n\n# a 3D plot with k value of 2 clusters\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot each cluster\nax.scatter(df0['avg_dip_duration'], df0['num_flux_dips'], df0['flux_std'], color='green', label='Cluster 0')\nax.scatter(df1['avg_dip_duration'], df1['num_flux_dips'], df1['flux_std'], color='red', label='Cluster 1')\n#ax.scatter(df2['avg_dip_duration'], df2['num_flux_dips'], df2['flux_std'], color='blue', label='Cluster 2')\n\n# Plot centroids\nax.scatter(km.cluster_centers_[:, 1], km.cluster_centers_[:, 0], km.cluster_centers_[:, 2], \n           color='purple', marker='X', s=100, label='Centroids')\n\n# Axis labels\nax.set_xlabel('Average Dip Duration')\nax.set_ylabel('Number of Flux Dips')\nax.set_zlabel('Flux Std Deviation')\nax.set_title('3D Clustering of TESS Light Curves')\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n5.6 Initial Data Insights\n\ndecided to go with k=3 as it seemed like mid way point bertween 2 and 4 where biggest change occured\n\nk=2\ncluster\n0    78\n1    22\nName: count, dtype: int64\n-----------------------\nk=3\ncluster\n0    73\n1    15\n2    12\nName: count, dtype: int64\n--------------------\nk= 4 \ncluster\n0    70\n3    14\n2     8\n1     8\nName: count, dtype: int64\n\n2D\n\nclusters are linear in nature, separation is present but seem alligned\nThere are zero values present which perhaps Zero values could represent low activity in stars. They are not missing values, but actual 0.0 values\nData does look strange and haven’t been able to figure out why ….\n\n3D plot\n\nx axis = avg fip duration\ny axis = num of flux dips\nz axis = flux variability\nCVlusters are well separtated in 3D plot, less so in 2D plot\ncentroids are where we would expect - indicating the average value for the set of clusters\n\n\n5.7 Plot Kmeans++ data\n\n# Perform KMeans++ clustering\nkmeans = KMeans(n_clusters=2, init='k-means++', random_state=42)\nkmeans.fit(x_df[[\"num_flux_dips\", \"avg_dip_duration\"]])\n\n# Get centroids\ncentroids = kmeans.cluster_centers_\n\n# Add cluster labels to DataFrame\nx_df[\"kmeans_pp_cluster\"] = kmeans.labels_\n\n# Plot\nplt.figure(figsize=(10, 8))\nplt.title(\"K-means++ Clustering with Centroids and 3 Clusters\")\nplt.xlabel(\"Number of Flux Dips\")\nplt.ylabel(\"Average Dip Duration\")\n\nplt.scatter(\n    x_df[\"num_flux_dips\"], \n    x_df[\"avg_dip_duration\"], \n    c=kmeans.labels_, \n    cmap='viridis', \n    s=50, alpha=0.6\n)\nplt.scatter(\n    centroids[:, 0], \n    centroids[:, 1], \n    c='red', \n    s=100, \n    marker='X', \n    label='Centroids'\n)\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n5.7 Calculate and Display the Silhouette values\nthe silhouette value can help us evaluate the quality of clustering. It may determine how well data points fit within their assigned clusters and whether the number of clusters is optimal\n\n\nfeatures = ['num_flux_dips', 'avg_dip_duration', 'flux_std']\ndf_for_clustering = x_df[features]\n\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nsilhouette_avg = []\n\nfor num_clusters in range_n_clusters:\n    # Initialize KMeans++\n    kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n    cluster_labels = kmeans.fit_predict(df_for_clustering)\n    \n    # Calculate average silhouette score\n    score = silhouette_score(df_for_clustering, cluster_labels)\n    silhouette_avg.append(score)\n    print(f\"For n_clusters = {num_clusters}, the average silhouette_score is: {score:.4f}\")\n\n# Plot the silhouette scores for each cluster count\nplt.figure(figsize=(8, 5))\nplt.plot(range_n_clusters, silhouette_avg, marker='o')\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Analysis for Optimal k\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nFor n_clusters = 2, the average silhouette_score is: 0.7577\nFor n_clusters = 3, the average silhouette_score is: 0.7044\nFor n_clusters = 4, the average silhouette_score is: 0.7463\nFor n_clusters = 5, the average silhouette_score is: 0.7143\nFor n_clusters = 6, the average silhouette_score is: 0.7146\nFor n_clusters = 7, the average silhouette_score is: 0.7201\nFor n_clusters = 8, the average silhouette_score is: 0.7366\n\n\n\n\n\n\n\n\n\n5.9 Alternative silhouette data plot\n\n# Ensure you use only the feature columns\nX = x_df[[\"avg_dip_duration\", \"num_flux_dips\", \"flux_std\"]].values\nrange_n_clusters = [2, 3, 4, 5]\n\nfor n_clusters in range_n_clusters:\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    clusterer = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n    cluster_labels = clusterer.fit_predict(X)\n\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(f\"For n_clusters = {n_clusters}, average silhouette_score = {silhouette_avg:.4f}\")\n\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        ith_vals = sample_silhouette_values[cluster_labels == i]\n        ith_vals.sort()\n        size = ith_vals.shape[0]\n        y_upper = y_lower + size\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_vals, facecolor=color, edgecolor=color, alpha=0.7)\n        ax1.text(-0.05, y_lower + 0.5 * size, str(i))\n        y_lower = y_upper + 10\n\n    ax1.set_title(\"The Silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The Silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n    ax1.set_yticks([])\n    ax1.set_xticks(np.linspace(-0.1, 1.0, 6))\n\n    # 2D cluster plot (first two features)\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\")\n\n    centers = clusterer.cluster_centers_\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o', c='white', alpha=1, s=200, edgecolor='k')\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker=f\"${i}$\", alpha=1, s=50, edgecolor='k')\n\n    ax2.set_title(\"The Visualization of the clusteered data (2 features)\")\n    ax2.set_xlabel(\"Average Dip Duration\")\n    ax2.set_ylabel(\"Number of Flux Dips\")\n\n    plt.suptitle(f\"Silhouette analysis for KMeans with n_clusters = {n_clusters}\", fontsize=14, fontweight='bold')\n    plt.show()\n\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nFor n_clusters = 2, average silhouette_score = 0.7577\n\n\n\n\n\n\n\n\n\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nFor n_clusters = 3, average silhouette_score = 0.7044\n\n\n\n\n\n\n\n\n\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nFor n_clusters = 4, average silhouette_score = 0.7463\n\n\n\n\n\n\n\n\n\nc:\\Users\\eamon\\anaconda3\\envs\\assignment-a\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nFor n_clusters = 5, average silhouette_score = 0.7143\n\n\n\n\n\n\n\n\n\n5.8 Data Insights from silhouette plots\n\nbest value is 1 =&gt; data point is very compact\nworst value is -1 = vlaues may have been assigned to the wrong clusters\nvalues of zero =&gt; overlapping clusters\nthe horizontal coloured irregular bars represents clusters and the width of the score represent a silhuette score\nred dashed line gives average silhouette score\n\nFor n_clusters = 2, the average silhouette_score is: 0.7577\nFor n_clusters = 3, the average silhouette_score is: 0.7044\nFor n_clusters = 4, the average silhouette_score is: 0.7463\nFor n_clusters = 5, the average silhouette_score is: 0.7143\nFor n_clusters = 6, the average silhouette_score is: 0.7146\nFor n_clusters = 7, the average silhouette_score is: 0.7201\n\nk=2 has thre highest score at .75 =&gt; this is our best k value\nk = 4 and 8 preform reasonably well also with high values\nk=3 does nto perform well\n\n\nStep 6 - Conclusion and Learnings\n\nThe data initially performed very well. All graphs and analysis as went displayed and looked as I had expected.\nIn the clustering phase the data may be performgin fine but the graphs do not look as I expected, althoguh as thsi is my firsttime workign with scientific dat oin this scale perhaps my expectations of what the data should lok like is incorrect. It may very well be performing as expected….\nThe hope was we would see cluster of data where dips occurred at the same frequency intervals. However that was not really was we observed in our clustering analysis\nWas our dataset big enough to spot a planet transitioning across a star.I do not know the answer to this. Repeating the process with a larger dataset may give better results. Perhaps the period of transition of a planet across a star is longer on this sector.\nData in the clustering phase didn’t Noy quite sure how to interpret the results and plots of the clustering.\n\na lot of zero values or near zero values. This mayh have been how I dealt with the outliers MinMaxScaler and Winsorising\n\nDBScan was run also on the and the values below were determined but it was hard to put an interpretation on them. There is valid clusters present but again the data looked a bit off.\n\neps ~ 0.15\nminPts ~ 4\n\n\ndid we use enough data. We could take a larger set fo data and from other sectors. There are challenges with this but the. Aggregating the compute or sharing the analysis is a good approach to spread the burden.\nI had initially thought the complexity of the dat coudl be overwhelming but the data itself is not overwhelmign once yuo overcome the intial shock of the column names and understanding their meaning, or mostly understanding their meaning.\nThere was a lot of learning around hgwo the scientific community process data. Data Analysis is a key part of scientific work. Identifyign efficient ways to standardise and\n\n\n\nChallenges\n\nThere is no shortage of data. However it is diffficult and time consuming finding the correct data and gaining an understanding of it. Thats a skill in itself\nI need to work with the data for an extended period to gain good insight into it and how to manage it\nProcessing time\n\nwe used a subset of 100 files - each of which contained approx 12K records. The amount of data we did not use was many multiples of this\nThis resulted in long processing times for some tasks\n\n\n\n\n\nImprovements\n\nget mroe data points to cross check based on light curve per star such as the duration of the dips, how consistenmt the dip is i.e. depth, how long it lasts for (may be able to get diameter of planet based on duration of dip)\nDBScan was run also on the and the values below were determined but it was hard to put an interpretation on them. There is valid clusters present but again the data looked a bit off.\n\neps ~ 0.15\nminPts ~ 4\n\n\nMore compute power. There is a lot of data to process, even our small subset of data\nNeed to do process this type of data regularly to gain domain knowledge, tips and tricks for dealing with it.\nPerform on a different sector over sa different time and se if it is any different.\nPerform in same sector but differnt rime period and see data performs differently.\nI will be quicker next time…\n\n\n\nAppendix - References\n\n[1] [methods for detecting dips in curves - LightKurve package] &gt; https://lightkurve.github.io/lightkurve/tutorials/3-science-examples/exoplanets-identifying-transiting-planet-signals.html\n[2][Kepler: A Search for Terrestrial Planets](https://archive.stsci.edu/files/live/sites/mast/files/home/missions-and-data/kepler/_documents/archive_manual.pdf)\n[3][Astropy - Installation](https://docs.astropy.org/en/stable/install.html)\n[4] INstallation"
  }
]